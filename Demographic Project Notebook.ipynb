{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Loading in Data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import types\nimport pandas as pd\nimport numpy as np\nfrom botocore.client import Config\nimport ibm_boto3\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport requests # library to handle requests\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import NearestNeighbors\nimport json\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nbody = client_1ac46fe64048461aa02020f0a715fcf7.get_object(Bucket='thequotsimplequotchangeoftheunion-donotdelete-pr-bxu3clanfhxujd',Key='ACSST5Y2018.S0101_data_with_overlays_2020-04-15T174250.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nACS_2018 = pd.read_csv(body)\nACS_2018.head()\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nbody = client_1ac46fe64048461aa02020f0a715fcf7.get_object(Bucket='thequotsimplequotchangeoftheunion-donotdelete-pr-bxu3clanfhxujd',Key='Poverty Data 2010.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nPoverty_2010 = pd.read_csv(body)\nPoverty_2010.head()\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nbody = client_1ac46fe64048461aa02020f0a715fcf7.get_object(Bucket='thequotsimplequotchangeoftheunion-donotdelete-pr-bxu3clanfhxujd',Key='Poverty Data 2018.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nPoverty_2018 = pd.read_csv(body)\nPoverty_2018.head()\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Removing that top row of the ACS dataframes that are just description filler, not data\n\nACS_2018 = ACS_2018.iloc[1:]\nACS_2010 = ACS_2010.iloc[1:]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Cleaning and Organizing"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#renaming columns in dfs\n\nACS_2018.rename(columns={'S0101_C01_001E': 'Total Population'}, inplace = True)\nACS_2018.rename(columns={'S0101_C03_001E': 'Total Male Population'}, inplace = True)\nACS_2018.rename(columns={'S0101_C05_001E': 'Total Female Population'}, inplace = True)\n\nACS_2010.rename(columns={'S0101_C01_001E': 'Total Population'}, inplace = True)\nACS_2010.rename(columns={'S0101_C02_001E': 'Total Male Population'}, inplace = True)\nACS_2010.rename(columns={'S0101_C03_001E': 'Total Female Population'}, inplace = True)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#renaming columns in dfs\n\nPoverty_2010.rename(columns={'All Ages in Poverty Percent': 'Poverty Percent'}, inplace = True)\nPoverty_2010.rename(columns={'Median Household Income in Dollars': 'MHI'}, inplace = True)\n\nPoverty_2018.rename(columns={'All Ages in Poverty Percent': 'Poverty Percent'}, inplace = True)\nPoverty_2018.rename(columns={'Median Household Income in Dollars': 'MHI'}, inplace = True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#looking at two counties that have changed in the 10 year period: Shannon County SD, and Wade Hampton County AK\nACS_2010[ACS_2010['NAME'] == 'Shannon County, South Dakota']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ACS_2018[ACS_2018['NAME'] == 'Oglala Lakota County, South Dakota'] #Shannon County was renamed Oglala Lakota County, and the GEO_ID has also changed"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "ACS_2010[ACS_2010['NAME'] == 'Wade Hampton Census Area, Alaska']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "ACS_2018[ACS_2018['NAME'] == 'Kusilvak Census Area, Alaska'] #Wade Hamptom (2010) has been replaced with Kusilvak Census Area, also of a different geoid\n\n#I kept the 2018 names, so we don't need to worry about changing the whole 2010 info. So we just need to change the GEO_ID 2010 version to the 2018 GEO_ID for these individual counties."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Poverty_2010[Poverty_2010['State / County Name'] == 'Shannon County (SD)']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Poverty_2018[Poverty_2018['State / County Name'] == 'Oglala Lakota County (SD)'] #ok, so 46102 is the county ID we need to switch the 2010 data to"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Poverty_2010.loc[Poverty_2010['State / County Name'] == 'Wade Hampton Census Area (AK)']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "Poverty_2018[Poverty_2018['State / County Name'] == 'Kusilvak Census Area (AK)'] #so we want the 2010 county id to be 2158"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "#Pre-emptively converting those two counties 2010 GEOIDs to their future, 2018 ones. This way we can keep track of them as the same county in future datasets, even as their names change\n\n#ACS Dataset\n\n#first, changing Shannon County ID to its future County ID in 2018\nACS_2010.loc[ACS_2010['GEO_ID'] == '0500000US46113', 'GEO_ID'] = '0500000US46102'\n\n#now, changing Wade Hampton Census Area County ID to its future County ID in 2018\nACS_2010.loc[ACS_2010['GEO_ID'] == '0500000US02270', 'GEO_ID']  = '0500000US02158'"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Poverty Dataset\n\n#first, changing Shannon County ID to its future County ID in 2018\nPoverty_2010.loc[Poverty_2010['State / County Name'] == 'Shannon County (SD)', 'County ID'] = 46102\n\n#now, changing Wade Hampton Census Area County ID to its future County ID in 2018\nPoverty_2010.loc[Poverty_2010['State / County Name'] == 'Wade Hampton Census Area (AK)', 'County ID'] = 2158"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#ACS 2018 and 2010: Need to segment out their GEO Ids\n\nACS_2018['County ID'] = ACS_2018['GEO_ID'].str.slice(9)\nACS_2010['County ID'] = ACS_2010['GEO_ID'].str.slice(9)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Taking only the counties for the Poverty datasets\n\nPoverty_2010 = Poverty_2010[Poverty_2010['State / County Name'].str.contains('\\(',case= True)]\nPoverty_2018 = Poverty_2018[Poverty_2018['State / County Name'].str.contains('\\(',case= True)]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "Poverty_2018.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#lets create a dataset with just the features we want\n#we need to remember which data is from which year though\n#check the meta-data files for a better look at which columsn correspond to what variable\n\n#For ACS Features\nFeatures_A2010 = ['GEO_ID', 'County ID', 'NAME', 'Total Population', 'Total Male Population', 'Total Female Population']\nFeatures_A2018 = ['GEO_ID', 'County ID', 'NAME', 'Total Population', 'Total Male Population', 'Total Female Population']\n\n\n#For Poverty Features\nFeatures_P2010 = ['County ID', 'State / County Name', 'Poverty Percent', 'MHI']\nFeatures_P2018 = ['County ID', 'State / County Name', 'Poverty Percent', 'MHI']"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Ok, I just want to check that those feature pulls are working alright"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ACS_2018[Features_A2018].head() #cool, the Features_A2018 works"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ACS_2010[Features_A2010].head() #cool, the Features_A2010 works"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "Poverty_2010[Features_P2010].head() #cool, the Features_P2010 works"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "Poverty_2018[Features_P2018].head() #cool, the Features_P2018 works"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Great, so I can fully segment these into new dataframes now"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfP_2010 = Poverty_2010[Features_P2010]\ndfP_2018 = Poverty_2018[Features_P2018]\n\ndfA_2010 = ACS_2010[Features_A2010]\ndfA_2018 = ACS_2018[Features_A2018]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "##Checking the datasets\n\n#dfP_2010.head()\ndfP_2018.head()\n#dfA_2010.head()\n#dfA_2018.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#Splitting the 'County, State' format of the ACS datasets into two separate columns: County and State\n\nsplitCol = dfA_2018['NAME'].str.split(\",\", expand = True)\ndfA_2018['County'] = splitCol[0]\ndfA_2018['State'] = splitCol[1]\ndfA_2018.drop(columns = ['NAME'], inplace = True)\n\nsplitCol = dfA_2010['NAME'].str.split(\",\", expand = True)\ndfA_2010['County'] = splitCol[0]\ndfA_2010['State'] = splitCol[1]\ndfA_2010.drop(columns = ['NAME'], inplace = True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "##Checking the datasets\n\n#dfP_2010.head()\n#dfP_2018.head()\ndfA_2010.head()\n#dfA_2018.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#merge the Poverty to Poverty sets first, ACS to ACS. Both via the geo codes\n\n#poverty 2010 to 2018 merge\ndfP = pd.merge(left = dfP_2010, right = dfP_2018, on = 'County ID', suffixes = (' 2010', ' 2018'))\n\n#ACS 2010 to 2018 merge\ndfA = pd.merge(left = dfA_2010, right = dfA_2018, on = 'GEO_ID', suffixes = (' 2010', ' 2018'))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfP.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfA.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Dealing with each dataset now, creating the change variables"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**dfA** \n> needs change metrics between: Total population, Male population, Female population"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfA['Total Population 2018'] = pd.to_numeric(dfA['Total Population 2018'])\ndfA['Total Population 2010'] = pd.to_numeric(dfA['Total Population 2010'])\ndfA['Total Male Population 2018'] = pd.to_numeric(dfA['Total Male Population 2018'])\ndfA['Total Male Population 2010'] = pd.to_numeric(dfA['Total Male Population 2010'])\ndfA['Total Female Population 2018'] = pd.to_numeric(dfA['Total Female Population 2018'])\ndfA['Total Female Population 2010'] = pd.to_numeric(dfA['Total Female Population 2010'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfA['% Total Population Change']  = round((dfA['Total Population 2018'] - dfA['Total Population 2010'])/(dfA['Total Population 2010'])*100, 2)\ndfA['% Male Population Change']  = round((dfA['Total Male Population 2018'] - dfA['Total Male Population 2010'])/(dfA['Total Male Population 2010'])*100, 2)\ndfA['% Female Population Change']  = round((dfA['Total Female Population 2018'] - dfA['Total Female Population 2010'])/(dfA['Total Female Population 2010'])*100, 2)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfA.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**dfP**\n> needs change metrics between: MHI, Poverty"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#getting rid of the '$' and ','in MHI data\ndfP['MHI 2010'] = dfP['MHI 2010'].str.slice(1)\ndfP['MHI 2010'] = dfP['MHI 2010'].replace(',', '', regex = True)\ndfP['MHI 2018'] = dfP['MHI 2018'].str.slice(1)\ndfP['MHI 2018'] = dfP['MHI 2018'].replace(',', '', regex = True)\n\n#changing to numerics for later calculations\ndfP['Poverty Percent 2010'] = pd.to_numeric(dfP['Poverty Percent 2010'])\ndfP['Poverty Percent 2018'] = pd.to_numeric(dfP['Poverty Percent 2018'])\ndfP['MHI 2010'] = pd.to_numeric(dfP['MHI 2010'])\ndfP['MHI 2018'] = pd.to_numeric(dfP['MHI 2018'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfP['% Poverty Change'] = dfP['Poverty Percent 2018'] - dfP['Poverty Percent 2010']\ndfP['% MHI Change'] = round((dfP['MHI 2018'] - dfP['MHI 2010'])/(dfP['MHI 2010'])*100, 2)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "dfP.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Final Dataset"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Merge the datasets, check that states and counties are lining up properly, then clean"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfA['County ID'] = pd.to_numeric(dfA['County ID 2010'])\n\ndf = pd.merge(left = dfA, right = dfP, left_on = 'County ID', right_on = 'County ID')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#just checking that the county 2010, state 2010, county 2018, state 2018, and State/county 2010 and 2018 are lining up\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Minor cleaning\n\ndf.drop(['State / County Name 2010', 'State / County Name 2018', 'County 2010', 'State 2010', 'County ID 2010'], axis = 1, inplace = True)\n\ndf.rename(columns = {'County 2018': 'County'}, inplace = True)\ndf.rename(columns = {'State 2018': 'State'}, inplace = True)\n\ndf['State'] = df['State'].str.lstrip()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def movecol(df, cols_to_move=[], ref_col='', place='After'):\n    \n    cols = df.columns.tolist()\n    if place == 'After':\n        seg1 = cols[:list(cols).index(ref_col) + 1]\n        seg2 = cols_to_move\n    if place == 'Before':\n        seg1 = cols[:list(cols).index(ref_col)]\n        seg2 = cols_to_move + [ref_col]\n    \n    seg1 = [i for i in seg1 if i not in seg2]\n    seg3 = [i for i in cols if i not in seg1 + seg2]\n    \n    return(df[seg1 + seg2 + seg3])\n\n#forma to use the movecol function\n#df = movecol(df, \n#             cols_to_move=['col1','col2'], \n#             ref_col='col3',\n#             place='After' or 'Before')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#moving some columns around\n#is using a whole new function overkill here? Absolutely. But its good practice for me to use. This is not my function btw, credit to Adam Ross Nelson.\n\ndf = movecol(df, \n             cols_to_move = ['County ID', 'County', 'State'],\n             ref_col = 'Total Population 2010',\n             place = 'Before')\n\ndf = movecol(df, \n             cols_to_move = ['% Total Population Change', '% Male Population Change', '% Female Population Change'],\n             ref_col = '% Poverty Change',\n             place = 'Before')\n\ndf = movecol(df, \n             cols_to_move = ['Poverty Percent 2010', 'MHI 2010'],\n             ref_col = 'Total Female Population 2010',\n             place = 'After')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df.describe()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Analyses"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Ok, now that I have the data I can do some analyses. I want to see clusters of:\n> *what counties were similar in 2010?* -- using raw, standardized data \\\n> *what counties are similar now in 2018?* -- using raw, standardized data \\\n> *what counties changes in similar ways?* -- using the % change metrics"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.describe()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Snapshot Analyses:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#2010\nFeatures_2010 = ['Total Population 2010','MHI 2010']\n\n#2018\nFeatures_2018 = ['Total Population 2018', 'MHI 2018']\n\n#Delta\nFeatures_delta = ['% Total Population Change', '% MHI Change']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.scatter(df['Total Population 2010'], df['MHI 2010'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.scatter(df['Total Population 2018'], df['MHI 2018'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Lets see what the min/max, stdev looks like without it those huge counties (E.g. LA). Then we can use those more descriptive stats and apply it to the original dataframe--in effect, dodging the outliers without removing its data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df[df['Total Population 2010'] > 5000000] #so LA County and Cook County seem to be those big population centres"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_Otl = df[(df.GEO_ID != '0500000US06037')] #Removing LA County\ndf_Otl = df_Otl[(df_Otl.GEO_ID != '0500000US17031')] #Removing Cook County\ndf_Otl"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.describe()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_Otl.describe() #see the change in the population metrics?"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.scatter(df_Otl['Total Population 2010'], df_Otl['MHI 2010'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now, to quantile the upper, middle, and lower thirds for both population and mhi of this new outlier-less dataframe. We'll apply those parameters to the original df later"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "Pop_parameters_2010 = df_Otl['Total Population 2010'].quantile([0, 0.33, 0.66, 1])\nPop_parameters_2010"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.scatter(df_Otl['Total Population 2010'], df_Otl['MHI 2010'])\n\nplt.axvline(x=Pop_parameters_2010[.33])\nplt.axvline(x=Pop_parameters_2010[.66])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "So we can see that quantiles aren't super useful here because the data is so close. Lets see if a split across the range is clearer."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Minimum Population =', df_Otl['Total Population 2010'].min(), 'Maximum Population = ', df_Otl['Total Population 2010'].max())"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pop10_third = (((df_Otl['Total Population 2010'].max()) - (df_Otl['Total Population 2010'].min()))/3)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.scatter(df_Otl['Total Population 2010'], df_Otl['MHI 2010'])\n\nplt.axvline(x= pop10_third)\nplt.axvline(x= 2*pop10_third)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This seems to work out a bit better. Lets try the MHI too"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "pop10_third = (((df_Otl['Total Population 2010'].max()) - (df_Otl['Total Population 2010'].min()))/3)\nMHI10_third = (((df_Otl['MHI 2010'].max()) - (df_Otl['MHI 2010'].min()))/3)\n\nplt.scatter(df_Otl['Total Population 2010'], df_Otl['MHI 2010'])\n\nplt.axvline(x= pop10_third)\nplt.axvline(x= 2*pop10_third)\n\nplt.axhline(y= MHI10_third)\nplt.axhline(y= 2*MHI10_third)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "OK, so we have pop10_third and MHI10_third as decent markers for 2010 to apply to our full data later on. Lets quickly make the 2018 versions"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pop18_third = (((df_Otl['Total Population 2018'].max()) - (df_Otl['Total Population 2018'].min()))/3)\nMHI18_third = (((df_Otl['MHI 2018'].max()) - (df_Otl['MHI 2018'].min()))/3)\n\nplt.scatter(df_Otl['Total Population 2018'], df_Otl['MHI 2018'])\n\nplt.axvline(x= pop18_third)\nplt.axvline(x= 2*pop18_third)\n\nplt.axhline(y= MHI18_third)\nplt.axhline(y= 2*MHI18_third)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Iiiiiiii don't quite know what to do with the Change data yet. Its so generic (see graph) that I'm not convinced I can easily do this third-split with that data too. Lets hold on that for now."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "plt.scatter(df['% Total Population Change'], df['% MHI Change'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#adding in categorical variables to a new column based off of our outlier-less third ranges\ndef f(row):\n    if row['Total Population 2010'] < pop10_third:\n        val = 0\n    elif row['Total Population 2010'] > 2*pop10_third:\n        val = 2\n    else:\n        val = 1\n    return val\n\ndf['2010 Population Split'] = df.apply(f, axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['2010 Population Split'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def f(row):\n    if row['MHI 2010'] < MHI10_third:\n        val = 0\n    elif row['MHI 2010'] > 2*MHI10_third:\n        val = 2\n    else:\n        val = 1\n    return val\n\ndf['2010 MHI Split'] = df.apply(f, axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['2010 MHI Split'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def f(row):\n    if row['2010 Population Split'] == 0 and row['2010 MHI Split'] == 0: #small, poor\n        val = 0\n    elif row['2010 Population Split'] == 1 and row['2010 MHI Split'] == 0: #medium, poor\n        val = 1\n    elif row['2010 Population Split'] == 2 and row['2010 MHI Split'] == 0: #large, poor\n        val = 2\n    elif row['2010 Population Split'] == 0 and row['2010 MHI Split'] == 1: #small, medium\n        val = 3\n    elif row['2010 Population Split'] == 1 and row['2010 MHI Split'] == 1: #medium, medium\n        val = 4\n    elif row['2010 Population Split'] == 2 and row['2010 MHI Split'] == 1: #large, medium\n        val = 5\n    elif row['2010 Population Split'] == 0 and row['2010 MHI Split'] == 2: #small, rich\n        val = 6\n    elif row['2010 Population Split'] == 1 and row['2010 MHI Split'] == 2: #medium, rich\n        val = 7\n    elif row['2010 Population Split'] == 2 and row['2010 MHI Split'] == 2: #large, rich\n        val = 8\n    else:\n        val = 100\n    return val\ndf['2010 Split'] = df.apply(f, axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['2010 Split'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def f(row):\n    if row['Total Population 2018'] < pop18_third:\n        val = 0\n    elif row['Total Population 2018'] > 2*pop18_third:\n        val = 2\n    else:\n        val = 1\n    return val\n\ndf['2018 Population Split'] = df.apply(f, axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['2018 Population Split'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def f(row):\n    if row['MHI 2018'] < MHI18_third:\n        val = 0\n    elif row['MHI 2018'] > 2*MHI18_third:\n        val = 2\n    else:\n        val = 1\n    return val\n\ndf['2018 MHI Split'] = df.apply(f, axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['2018 MHI Split'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def f(row):\n    if row['2018 Population Split'] == 0 and row['2018 MHI Split'] == 0: #small, poor\n        val = 0\n    elif row['2018 Population Split'] == 1 and row['2018 MHI Split'] == 0: #medium, poor\n        val = 1\n    elif row['2018 Population Split'] == 2 and row['2018 MHI Split'] == 0: #large, poor\n        val = 2\n    elif row['2018 Population Split'] == 0 and row['2018 MHI Split'] == 1: #small, medium\n        val = 3\n    elif row['2018 Population Split'] == 1 and row['2018 MHI Split'] == 1: #medium, medium\n        val = 4\n    elif row['2018 Population Split'] == 2 and row['2018 MHI Split'] == 1: #large, medium\n        val = 5\n    elif row['2018 Population Split'] == 0 and row['2018 MHI Split'] == 2: #small, rich\n        val = 6\n    elif row['2018 Population Split'] == 1 and row['2018 MHI Split'] == 2: #medium, rich\n        val = 7\n    elif row['2018 Population Split'] == 2 and row['2018 MHI Split'] == 2: #large, rich\n        val = 8\n    else:\n        val = 100\n    return val\ndf['2018 Split'] = df.apply(f, axis = 1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df['2018 Split'].value_counts()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### DBSCAN Analyses"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**2010 Clustering Snapshot**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Grabbing the features we want for the 2010 snapshot\nX = df[Features_2010]\n\n#taking out any NaNs, and standardizing the data.\nX = np.nan_to_num(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "X"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#finding reasonable DBSCAN input values\nneigh = NearestNeighbors(n_neighbors = 2)\n\nnbrs = neigh.fit(X)\n\ndistances, indices = nbrs.kneighbors(X)\n\ndistances = np.sort(distances, axis = 0)\n\ndistances = distances[:, 1]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.plot(distances) #our optimal value of epsilon is the point of maximum curvature\nplt.ylim((0,1)) #looks like epsilon should be about 0.2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.cluster import DBSCAN\nimport sklearn.utils\n\nsklearn.utils.check_random_state(1000)\n\n#Compute DBSCAN\ndb = DBSCAN(eps = .1, min_samples = 4).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype = bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from scipy import stats\nstats.describe(db.labels_)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.hist(db.labels_)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#insert the 2010 snapshot into the main df\ndf.insert(0, 'DBSCAN 2010 Cluster', db.labels_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**2018 Snapshot Clustering**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Grabbing the features we want for the 2018 snapshot\nX = df[Features_2018]\n\n#taking out any NaNs, and standardizing the data.\nX = np.nan_to_num(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "X"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#finding reasonable DBSCAN input values\nneigh = NearestNeighbors(n_neighbors = 2)\n\nnbrs = neigh.fit(X)\n\ndistances, indices = nbrs.kneighbors(X)\n\ndistances = np.sort(distances, axis = 0)\n\ndistances = distances[:, 1]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.plot(distances) #our optimal value of epsilon is the point of maximum curvature\nplt.ylim((0,1)) #looks like epsilon should be about 0.2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.cluster import DBSCAN\nimport sklearn.utils\n\nsklearn.utils.check_random_state(1000)\n\n#Compute DBSCAN\ndb = DBSCAN(eps = 0.2, min_samples = 5).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype = bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\nrealClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\nclusterNum = len(set(labels))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#insert the 2018 snapshot into the main df\ndf.insert(0, 'DBSCAN 2018 Cluster', db.labels_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**% Change Clustering**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Grabbing the features we want for the 2010 snapshot\nX = df[Features_delta]\n\n#taking out any NaNs, and standardizing the data.\nX = np.nan_to_num(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#finding reasonable DBSCAN input values\nneigh = NearestNeighbors(n_neighbors = 2)\n\nnbrs = neigh.fit(X)\n\ndistances, indices = nbrs.kneighbors(X)\n\ndistances = np.sort(distances, axis = 0)\n\ndistances = distances[:, 1]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "plt.plot(distances) #our optimal value of epsilon is the point of maximum curvature\nplt.ylim((0,1)) #looks like epsilon should be about 0.25"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.cluster import DBSCAN\nimport sklearn.utils\n\nsklearn.utils.check_random_state(1000)\n\n#Compute DBSCAN\ndb = DBSCAN(eps = 0.3, min_samples = 5).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype = bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\nrealClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\nclusterNum = len(set(labels))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#insert the change clustering into the main df\ndf.insert(0, 'DBSCAN Change Cluster', db.labels_)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "df['DBSCAN Change Cluster'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df['DBSCAN 2018 Cluster'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df['DBSCAN 2010 Cluster'].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "df.head() #nice, all of our clusters are showing up in the dataframe"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### KMeans Analyses"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "2010 Snapshot"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Grabbing the features we want for the 2010 snapshot\nX = df[Features_2010]\n\n#taking out any NaNs, and standardizing the data.\nX = np.nan_to_num(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n#running iterative KMeans the analyses\nfor clusters in range (1, 12):\n    kmeans = KMeans(n_clusters = clusters, random_state = 0).fit(X)\n    unique, counts = np.unique(kmeans.labels_, return_counts = True)\n    print(dict(zip(unique, counts)))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "KMeans(n_clusters = 9, random_state = 0).fit(X)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#insert the 2010 clustering into the main df\ndf.insert(0, 'Kmeans 2010 Cluster', kmeans.labels_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "2018 Snapshot"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Grabbing the features we want for the 2010 snapshot\nX = df[Features_2018]\n\n#taking out any NaNs, and standardizing the data.\nX = np.nan_to_num(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n#running iterative KMeans the analyses\nfor clusters in range (1, 12):\n    kmeans = KMeans(n_clusters = clusters, random_state = 0).fit(X)\n    unique, counts = np.unique(kmeans.labels_, return_counts = True)\n    print(dict(zip(unique, counts)))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "KMeans(n_clusters = 9, random_state = 0).fit(X)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#insert the 2018 clustering into the main df\ndf.insert(0, 'Kmeans 2018 Cluster', kmeans.labels_)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Change Clusters"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Grabbing the features we want for the 2010 snapshot\nX = df[Features_delta]\n\n#taking out any NaNs, and standardizing the data.\nX = np.nan_to_num(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n#running iterative KMeans the analyses\nfor clusters in range (1, 12):\n    kmeans = KMeans(n_clusters = clusters, random_state = 0).fit(X)\n    unique, counts = np.unique(kmeans.labels_, return_counts = True)\n    print(dict(zip(unique, counts)))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "KMeans(n_clusters = 9, random_state = 0).fit(X)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#insert the 2010 clustering into the main df\ndf.insert(0, 'Kmeans Change Cluster', kmeans.labels_)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df.describe() #nothing looks too crazy, which is good. We have 16 groups in Change and 2018 clusters, 21 for 2010 clusters"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Visualization"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Showing the clusters--what characterizes each? What counties are in each?"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**2010**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df[['DBSCAN 2010 Cluster', 'Total Population 2010', 'Poverty Percent 2010', 'MHI 2010']].groupby(['DBSCAN 2010 Cluster']).mean()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df[['Kmeans 2010 Cluster', 'Total Population 2010', 'Poverty Percent 2010', 'MHI 2010']].groupby(['Kmeans 2010 Cluster']).mean()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**2018**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df[['DBSCAN 2018 Cluster', 'Total Population 2018', 'Poverty Percent 2018', 'MHI 2018']].groupby(['DBSCAN 2018 Cluster']).mean()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df[['Kmeans 2018 Cluster', 'Total Population 2018', 'Poverty Percent 2018', 'MHI 2018']].groupby(['Kmeans 2018 Cluster']).mean()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Change**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df[['DBSCAN Change Cluster', '% Total Population Change', '% Poverty Change', '% MHI Change']].groupby(['DBSCAN Change Cluster']).mean()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "df[['Kmeans Change Cluster', '% Total Population Change', '% Poverty Change', '% MHI Change']].groupby(['Kmeans Change Cluster']).mean()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Maps"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "!pip install folium\nimport folium\nimport os\nimport json"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#creating a basic map of the USA\n\nlatitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#download geojson file of all counties in USA. From my github--i converted that file from the\n\n!wget --quiet https://github.com/hajnoszian/ForFutureReference/raw/master/cb_2018_us_county_5m.geojson"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install geopandas"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import geopandas as gpd\ncounty_geo = 'cb_2018_us_county_5m.geojson'\ngeoPdata = gpd.read_file(county_geo)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfGEO = pd.merge(\n    left = geoPdata,\n    right = df,\n    left_on = 'AFFGEOID', right_on = 'GEO_ID')\ndfGEO.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dfGEO.to_file('full_county_data.geojson', driver = 'GeoJSON')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "county_geo = 'full_county_data.geojson'\n\n#to check the key on variables\n#with open(county_geo) as f:\n#    data = json.load(f)\n#data "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This first one is a test to see if the counties map on correctly"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'Total Population 2010'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'YlOrRd',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"2010 Total Population\"\n).add_to(USA_map)\nUSA_map\n#now to add descriptives"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Maps of 2010"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "> Population \\\n> MHI \\\n> Clustering \\\n> Split"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Population 2010**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\n\nbins = list(df['Total Population 2010'].quantile([0, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 1])) #this is to break up the legend into more discernible color groupings\n\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'Total Population 2010'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'Blues',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"2010 Total Population\",\n    bins = bins,\n).add_to(USA_map)\nUSA_map\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**MHI 2010**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\nbins = list(df['MHI 2010'].quantile([0, 0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 1])) #this is to break up the legend into more discernible color groupings\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'MHI 2010'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'BuGn',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"MHI 2010\",\n    bins = bins\n).add_to(USA_map)\nUSA_map"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Clusters 2010**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\n#plotting the boundaries\nchoropleth = folium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'Kmeans 2010 Cluster'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'Set1',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    highlight = True,\n    legend_name = \"Kmeans Clusters\"\n).add_to(USA_map)\n\n#adding labels\nstyle_function = \"font-size: 15px; font-weight: bold\"\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'Kmeans 2010 Cluster', 'Total Population 2010', 'MHI 2010'],\n                                   aliases = ['County Name', 'Cluster', 'Population', 'MHI'],\n                                   labels = True))\n\nUSA_map"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\n#plotting the boundaries\nchoropleth = folium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'DBSCAN 2010 Cluster'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'Set1',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    highlight = True,\n    legend_name = \"DBSCAN Clusters\"\n).add_to(USA_map)\n\n#adding labels\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'DBSCAN 2010 Cluster', 'Total Population 2010', 'MHI 2010'],\n                                   aliases = ['County Name', 'Cluster', 'Population', 'MHI'],\n                                   labels = True))\n\nUSA_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Maps of 2018"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "> Population \\\n> MHI \\\n> Clustering \\\n> Split"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Population 2018**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\nbins = list(df['Total Population 2018'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1])) #this is to break up the legend into more discernible color groupings\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'Total Population 2018'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'PuBu',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"2018 Total Population\",\n    bins = bins\n).add_to(USA_map)\nUSA_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**MHI 2018**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\nbins = list(df['MHI 2018'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1])) #this is to break up the legend into more discernible color groupings\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'MHI 2018'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'BuGn',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"MHI 2018\",\n    bins = bins\n).add_to(USA_map)\nUSA_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Clusters 2018**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\n#plotting the boundaries\nchoropleth = folium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'Kmeans 2018 Cluster'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'Set1',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    highlight = True,\n    legend_name = \"Kmeans Clusters\"\n).add_to(USA_map)\n\n#adding labels\n\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'Kmeans 2018 Cluster', 'Total Population 2018', 'MHI 2018'],\n                                   aliases = ['County Name', 'Cluster', 'Population', 'MHI'],\n                                   labels = True))\n\nUSA_map"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\n#plotting the boundaries\nchoropleth = folium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', 'DBSCAN 2018 Cluster'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'Set1',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    highlight = True,\n    legend_name = \"DBSCAN Clusters\"\n).add_to(USA_map)\n\n#adding labels\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'DBSCAN 2018 Cluster', 'Total Population 2018', 'MHI 2018'],\n                                   aliases = ['County Name', 'Cluster', 'Population', 'MHI'],\n                                   labels = True))\n\nUSA_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Change from 2010 to 2018"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Population Change**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\nbins = list(df['% Total Population Change'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1])) #this is to break up the legend into more discernible color groupings\n\n\n#plotting the boundaries\nchoropleth = folium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', '% Total Population Change'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'RdBu',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    highlight = True,\n    legend_name = \"DBSCAN Clusters\"\n).add_to(USA_map)\n\n#adding labels\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'Total Population 2010', 'Total Population 2018','% Total Population Change'],\n                                   aliases = ['County Name', '2010 Population', '2018 Population', 'Change'],\n                                   labels = True))\n\nUSA_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**MHI Change**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nlatitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\nbins = list(df['% MHI Change'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1])) #this is to break up the legend into more discernible color groupings\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', '% MHI Change'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'RdBu',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"% MHI Change\",\n    bins = bins\n).add_to(USA_map)\n\n#adding labels\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'MHI 2010', 'MHI 2018', '% MHI Change'],\n                                   aliases = ['County Name', 'MHI 2010', 'MHI 2018', 'Change'],\n                                   labels = True))\n\nUSA_map"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Poverty Change**"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "latitude = 37\nlongitude = -95\n\nUSA_map = folium.Map(location=[latitude, longitude], zoom_start = 4)\nUSA_map\n\nbins = list(df['% Poverty Change'].quantile([0, 0.2, 0.4, 0.6, 0.8, 1])) #this is to break up the legend into more discernible color groupings\n\n#plotting the boundaries\nfolium.Choropleth(\n    geo_data = county_geo,\n    data = df,\n    columns = ['GEO_ID', '% Poverty Change'],\n    key_on = 'feature.properties.AFFGEOID',\n    fill_color = 'RdBu',\n    fill_opacity = 0.6,\n    line_opacity= .4,\n    legend_name = \"% Poverty Change\",\n    bins = bins\n).add_to(USA_map)\n\nchoropleth.geojson.add_child(\n    folium.features.GeoJsonTooltip(fields = ['County', 'MHI 2010', 'MHI 2018', '% Poverty Change'],\n                                   aliases = ['County Name', 'MHI 2010', 'MHI 2018', 'Change'],\n                                   labels = True))\nUSA_map"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}